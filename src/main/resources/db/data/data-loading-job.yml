# =====================================================================================
# Spring Batch Data Loading Job Configuration: data-loading-job.yml
# =====================================================================================
# Description: Orchestrates the complete data loading pipeline for 9 ASCII files
#              from the legacy VSAM system into PostgreSQL tables with exact
#              BigDecimal precision preservation and efficient bulk operations
# Author: Blitzy Agent - CardDemo Migration Team
# Version: CardDemo_v1.0-15-g27d6c6f-68
# Dependencies: All database migrations V1-V6, individual data reader configurations
# =====================================================================================
#
# This configuration implements:
# - Complete VSAM-to-PostgreSQL data migration workflow
# - Exact COBOL COMP-3 decimal precision preservation using BigDecimal
# - Chunk-based processing with 1000-record chunks per Section 0.3.1
# - Proper error handling and fault tolerance mechanisms
# - Referential integrity enforcement with foreign key constraints
# - High-performance bulk loading operations using PostgreSQL COPY
# - Comprehensive audit logging and progress monitoring
#
# Technical Specification References:
# - Section 0.1.2: Data precision mandate - exact COBOL COMP-3 equivalence
# - Section 0.2.3: File and path mapping - complete 9 ASCII file coverage
# - Section 0.3.1: Technical approach - chunk-based processing patterns
# - Section 6.2.2.1: Migration procedures - CSV/ASCII bulk loading operations
# =====================================================================================

spring:
  batch:
    # =====================================================================================
    # MAIN DATA LOADING JOB CONFIGURATION
    # =====================================================================================
    # Primary job definition orchestrating the complete data loading pipeline
    # for all 9 ASCII files with proper sequencing and dependency management
    job:
      data-loading-main:
        name: "DataLoadingMainJob"
        description: "Complete data loading pipeline for 9 ASCII files from VSAM to PostgreSQL"
        restartable: true
        incrementer: "RunIdIncrementer"
        
        # Job execution configuration
        execution:
          timeout: 14400000  # 4 hours maximum execution time
          concurrency: 3     # Maximum concurrent steps
          
        # Job-level listeners for monitoring and audit
        listeners:
          - type: "JobExecutionLoggingListener"
            log-level: "INFO"
            include-parameters: true
          - type: "JobMetricsListener"
            metrics-enabled: true
            prometheus-enabled: true
          - type: "JobAuditListener"
            audit-enabled: true
            audit-table: "BATCH_JOB_AUDIT"
        
        # =====================================================================================
        # STEP ORCHESTRATION CONFIGURATION
        # =====================================================================================
        # Defines the sequential execution flow ensuring proper data dependencies
        # Reference data loaded first, followed by core business data
        steps:
          
          # =====================================================================================
          # PHASE 1: REFERENCE DATA LOADING
          # =====================================================================================
          # Load reference data first to establish foreign key relationships
          
          # Step 1: Transaction Types Reference Data
          - name: "loadTransactionTypesStep"
            description: "Load transaction types from trantype.txt"
            next: "loadTransactionCategoriesStep"
            
            tasklet:
              type: "FlatFileToTableTasklet"
              reader-config: "classpath:db/data/reference-data-reader.yml#transaction-types"
              writer-config: "PostgreSQLBulkInsertWriter"
              
              # Chunk processing configuration
              chunk-size: 10  # Small reference data chunk
              commit-interval: 10
              
              # Error handling configuration
              skip-limit: 1
              skip-policy: "LimitSkipPolicy"
              retry-limit: 3
              
              # File processing configuration
              source-file: "classpath:db/data/ASCII/trantype.txt"
              target-table: "transaction_types"
              
              # Field mapping configuration
              field-mappings:
                - source-field: "transactionType"
                  target-column: "transaction_type"
                  type: "STRING"
                  validation: "notNull,size(min=2,max=2)"
                  
                - source-field: "typeDescription"
                  target-column: "type_description"
                  type: "STRING"
                  validation: "notNull,size(min=1,max=60)"
                  
                - source-field: "debitCreditFlag"
                  target-column: "debit_credit_indicator"
                  type: "BOOLEAN"
                  default: true
                  
                - source-field: "activeStatus"
                  target-column: "active_status"
                  type: "BOOLEAN"
                  default: true
                  
                - source-field: "createdAt"
                  target-column: "created_at"
                  type: "TIMESTAMP"
                  default: "CURRENT_TIMESTAMP"
                  
                - source-field: "updatedAt"
                  target-column: "updated_at"
                  type: "TIMESTAMP"
                  default: "CURRENT_TIMESTAMP"
          
          # Step 2: Transaction Categories Reference Data
          - name: "loadTransactionCategoriesStep"
            description: "Load transaction categories from trancatg.txt"
            next: "loadDisclosureGroupsStep"
            
            tasklet:
              type: "FlatFileToTableTasklet"
              reader-config: "classpath:db/data/reference-data-reader.yml#transaction-categories"
              writer-config: "PostgreSQLBulkInsertWriter"
              
              # Chunk processing configuration
              chunk-size: 20  # Transaction categories chunk
              commit-interval: 20
              
              # Error handling configuration
              skip-limit: 2
              skip-policy: "LimitSkipPolicy"
              retry-limit: 3
              
              # File processing configuration
              source-file: "classpath:db/data/ASCII/trancatg.txt"
              target-table: "transaction_categories"
              
              # Field mapping configuration
              field-mappings:
                - source-field: "transactionCategory"
                  target-column: "transaction_category"
                  type: "STRING"
                  validation: "notNull,size(min=4,max=4)"
                  
                - source-field: "categoryDescription"
                  target-column: "category_description"
                  type: "STRING"
                  validation: "notNull,size(min=1,max=60)"
                  
                - source-field: "parentTransactionType"
                  target-column: "parent_transaction_type"
                  type: "STRING"
                  validation: "notNull,size(min=2,max=2)"
                  
                - source-field: "activeStatus"
                  target-column: "active_status"
                  type: "BOOLEAN"
                  default: true
                  
                - source-field: "createdAt"
                  target-column: "created_at"
                  type: "TIMESTAMP"
                  default: "CURRENT_TIMESTAMP"
                  
                - source-field: "updatedAt"
                  target-column: "updated_at"
                  type: "TIMESTAMP"
                  default: "CURRENT_TIMESTAMP"
          
          # Step 3: Disclosure Groups Reference Data
          - name: "loadDisclosureGroupsStep"
            description: "Load disclosure groups from discgrp.txt with DECIMAL(5,4) precision"
            next: "loadCustomersStep"
            
            tasklet:
              type: "FlatFileToTableTasklet"
              reader-config: "classpath:db/data/reference-data-reader.yml#disclosure-groups"
              writer-config: "PostgreSQLBulkInsertWriter"
              
              # Chunk processing configuration
              chunk-size: 50  # Disclosure groups chunk
              commit-interval: 50
              
              # Error handling configuration
              skip-limit: 5
              skip-policy: "LimitSkipPolicy"
              retry-limit: 3
              
              # File processing configuration
              source-file: "classpath:db/data/ASCII/discgrp.txt"
              target-table: "disclosure_groups"
              
              # Field mapping configuration with BigDecimal precision
              field-mappings:
                - source-field: "groupId"
                  target-column: "group_id"
                  type: "STRING"
                  validation: "notNull,size(min=1,max=10)"
                  
                - source-field: "interestRate"
                  target-column: "interest_rate"
                  type: "BIGDECIMAL"
                  precision: 5
                  scale: 4
                  math-context: "DECIMAL128"
                  rounding-mode: "HALF_EVEN"
                  validation: "decimalMin(0.0001),decimalMax(9.9999)"
                  
                - source-field: "disclosureText"
                  target-column: "disclosure_text"
                  type: "STRING"
                  default: "Standard disclosure text for interest rate group"
                  validation: "size(max=1000)"
                  
                - source-field: "effectiveDate"
                  target-column: "effective_date"
                  type: "DATE"
                  default: "CURRENT_DATE"
                  
                - source-field: "activeStatus"
                  target-column: "active_status"
                  type: "BOOLEAN"
                  default: true
                  
                - source-field: "createdAt"
                  target-column: "created_at"
                  type: "TIMESTAMP"
                  default: "CURRENT_TIMESTAMP"
                  
                - source-field: "updatedAt"
                  target-column: "updated_at"
                  type: "TIMESTAMP"
                  default: "CURRENT_TIMESTAMP"
          
          # =====================================================================================
          # PHASE 2: CORE BUSINESS DATA LOADING
          # =====================================================================================
          # Load core business data with proper foreign key relationships
          
          # Step 4: Customer Data Loading
          - name: "loadCustomersStep"
            description: "Load customer data from custdata.txt with PII protection"
            next: "loadAccountsStep"
            
            tasklet:
              type: "FlatFileToTableTasklet"
              reader-config: "classpath:db/data/customer-data-reader.yml"
              writer-config: "PostgreSQLBulkInsertWriter"
              
              # Chunk processing configuration
              chunk-size: 100  # Customer data chunk size
              commit-interval: 100
              
              # Error handling configuration
              skip-limit: 10
              skip-policy: "LimitSkipPolicy"
              retry-limit: 3
              
              # File processing configuration
              source-file: "classpath:db/data/ASCII/custdata.txt"
              target-table: "customers"
              record-length: 312  # Customer record length
              
              # Field mapping configuration
              field-mappings:
                - source-field: "customerId"
                  target-column: "customer_id"
                  type: "STRING"
                  validation: "notNull,pattern(^[0-9]{9}$)"
                  
                - source-field: "firstName"
                  target-column: "first_name"
                  type: "STRING"
                  validation: "notNull,size(min=1,max=25)"
                  
                - source-field: "middleName"
                  target-column: "middle_name"
                  type: "STRING"
                  validation: "size(max=25)"
                  
                - source-field: "lastName"
                  target-column: "last_name"
                  type: "STRING"
                  validation: "notNull,size(min=1,max=25)"
                  
                - source-field: "addressLine1"
                  target-column: "address_line_1"
                  type: "STRING"
                  validation: "notNull,size(min=1,max=50)"
                  
                - source-field: "addressLine2"
                  target-column: "address_line_2"
                  type: "STRING"
                  validation: "size(max=50)"
                  
                - source-field: "city"
                  target-column: "city"
                  type: "STRING"
                  validation: "notNull,size(min=1,max=25)"
                  
                - source-field: "state"
                  target-column: "state"
                  type: "STRING"
                  validation: "notNull,size(min=2,max=2)"
                  
                - source-field: "zipCode"
                  target-column: "zip_code"
                  type: "STRING"
                  validation: "notNull,size(min=5,max=10)"
                  
                - source-field: "phoneNumber"
                  target-column: "phone_number"
                  type: "STRING"
                  validation: "size(max=15)"
                  
                - source-field: "governmentId"
                  target-column: "government_id"
                  type: "STRING"
                  validation: "notNull,size(min=9,max=11)"
                  
                - source-field: "dateOfBirth"
                  target-column: "date_of_birth"
                  type: "DATE"
                  validation: "notNull,past"
                  
                - source-field: "ficoScore"
                  target-column: "fico_score"
                  type: "INTEGER"
                  validation: "min(300),max(850)"
                  
                - source-field: "activeStatus"
                  target-column: "active_status"
                  type: "BOOLEAN"
                  default: true
                  
                - source-field: "createdAt"
                  target-column: "created_at"
                  type: "TIMESTAMP"
                  default: "CURRENT_TIMESTAMP"
                  
                - source-field: "updatedAt"
                  target-column: "updated_at"
                  type: "TIMESTAMP"
                  default: "CURRENT_TIMESTAMP"
          
          # Step 5: Account Data Loading
          - name: "loadAccountsStep"
            description: "Load account data from acctdata.txt with BigDecimal precision"
            next: "loadCardsStep"
            
            tasklet:
              type: "FlatFileToTableTasklet"
              reader-config: "classpath:db/data/account-data-reader.yml"
              writer-config: "PostgreSQLBulkInsertWriter"
              
              # Chunk processing configuration
              chunk-size: 50  # Account data chunk size
              commit-interval: 50
              
              # Error handling configuration
              skip-limit: 5
              skip-policy: "LimitSkipPolicy"
              retry-limit: 3
              
              # File processing configuration
              source-file: "classpath:db/data/ASCII/acctdata.txt"
              target-table: "accounts"
              record-length: 300  # Account record length
              
              # Field mapping configuration with BigDecimal precision
              field-mappings:
                - source-field: "accountId"
                  target-column: "account_id"
                  type: "STRING"
                  validation: "notNull,pattern(^[0-9]{11}$)"
                  
                - source-field: "customerId"
                  target-column: "customer_id"
                  type: "STRING"
                  validation: "notNull,pattern(^[0-9]{9}$)"
                  
                - source-field: "activeStatus"
                  target-column: "active_status"
                  type: "BOOLEAN"
                  default: true
                  
                - source-field: "currentBalance"
                  target-column: "current_balance"
                  type: "BIGDECIMAL"
                  precision: 12
                  scale: 2
                  math-context: "DECIMAL128"
                  rounding-mode: "HALF_EVEN"
                  validation: "decimalMin(-999999999.99),decimalMax(999999999.99)"
                  
                - source-field: "creditLimit"
                  target-column: "credit_limit"
                  type: "BIGDECIMAL"
                  precision: 12
                  scale: 2
                  math-context: "DECIMAL128"
                  rounding-mode: "HALF_EVEN"
                  validation: "decimalMin(0.00),decimalMax(999999999.99)"
                  
                - source-field: "cashCreditLimit"
                  target-column: "cash_credit_limit"
                  type: "BIGDECIMAL"
                  precision: 12
                  scale: 2
                  math-context: "DECIMAL128"
                  rounding-mode: "HALF_EVEN"
                  validation: "decimalMin(0.00),decimalMax(999999999.99)"
                  
                - source-field: "openDate"
                  target-column: "open_date"
                  type: "DATE"
                  validation: "notNull"
                  
                - source-field: "expirationDate"
                  target-column: "expiration_date"
                  type: "DATE"
                  validation: "future"
                  
                - source-field: "reissueDate"
                  target-column: "reissue_date"
                  type: "DATE"
                  
                - source-field: "currentCycleCredit"
                  target-column: "current_cycle_credit"
                  type: "BIGDECIMAL"
                  precision: 12
                  scale: 2
                  math-context: "DECIMAL128"
                  rounding-mode: "HALF_EVEN"
                  default: "0.00"
                  
                - source-field: "currentCycleDebit"
                  target-column: "current_cycle_debit"
                  type: "BIGDECIMAL"
                  precision: 12
                  scale: 2
                  math-context: "DECIMAL128"
                  rounding-mode: "HALF_EVEN"
                  default: "0.00"
                  
                - source-field: "addressZip"
                  target-column: "address_zip"
                  type: "STRING"
                  validation: "size(max=10)"
                  
                - source-field: "disclosureGroupId"
                  target-column: "disclosure_group_id"
                  type: "STRING"
                  validation: "size(max=10)"
                  
                - source-field: "createdAt"
                  target-column: "created_at"
                  type: "TIMESTAMP"
                  default: "CURRENT_TIMESTAMP"
                  
                - source-field: "updatedAt"
                  target-column: "updated_at"
                  type: "TIMESTAMP"
                  default: "CURRENT_TIMESTAMP"
          
          # Step 6: Card Data Loading
          - name: "loadCardsStep"
            description: "Load card data from carddata.txt with Luhn validation"
            next: "loadCardXrefStep"
            
            tasklet:
              type: "FlatFileToTableTasklet"
              reader-config: "classpath:db/data/card-data-reader.yml"
              writer-config: "PostgreSQLBulkInsertWriter"
              
              # Chunk processing configuration
              chunk-size: 100  # Card data chunk size
              commit-interval: 100
              
              # Error handling configuration
              skip-limit: 10
              skip-policy: "LimitSkipPolicy"
              retry-limit: 3
              
              # File processing configuration
              source-file: "classpath:db/data/ASCII/carddata.txt"
              target-table: "cards"
              record-length: 150  # Card record length
              
              # Field mapping configuration
              field-mappings:
                - source-field: "cardNumber"
                  target-column: "card_number"
                  type: "STRING"
                  validation: "notNull,pattern(^[0-9]{16}$),luhn"
                  
                - source-field: "accountId"
                  target-column: "account_id"
                  type: "STRING"
                  validation: "notNull,pattern(^[0-9]{11}$)"
                  
                - source-field: "customerId"
                  target-column: "customer_id"
                  type: "STRING"
                  validation: "notNull,pattern(^[0-9]{9}$)"
                  
                - source-field: "cardHolderName"
                  target-column: "card_holder_name"
                  type: "STRING"
                  validation: "notNull,size(min=1,max=50)"
                  
                - source-field: "expirationDate"
                  target-column: "expiration_date"
                  type: "DATE"
                  validation: "notNull,future"
                  
                - source-field: "activeStatus"
                  target-column: "active_status"
                  type: "BOOLEAN"
                  default: true
                  
                - source-field: "createdAt"
                  target-column: "created_at"
                  type: "TIMESTAMP"
                  default: "CURRENT_TIMESTAMP"
                  
                - source-field: "updatedAt"
                  target-column: "updated_at"
                  type: "TIMESTAMP"
                  default: "CURRENT_TIMESTAMP"
          
          # Step 7: Card Cross-Reference Data Loading
          - name: "loadCardXrefStep"
            description: "Load card cross-reference data from cardxref.txt"
            next: "loadTransactionsStep"
            
            tasklet:
              type: "FlatFileToTableTasklet"
              reader-config: "classpath:db/data/card-data-reader.yml#card-xref"
              writer-config: "PostgreSQLBulkInsertWriter"
              
              # Chunk processing configuration
              chunk-size: 100  # Card xref chunk size
              commit-interval: 100
              
              # Error handling configuration
              skip-limit: 10
              skip-policy: "LimitSkipPolicy"
              retry-limit: 3
              
              # File processing configuration
              source-file: "classpath:db/data/ASCII/cardxref.txt"
              target-table: "card_cross_reference"
              
              # Field mapping configuration
              field-mappings:
                - source-field: "cardNumber"
                  target-column: "card_number"
                  type: "STRING"
                  validation: "notNull,pattern(^[0-9]{16}$)"
                  
                - source-field: "accountId"
                  target-column: "account_id"
                  type: "STRING"
                  validation: "notNull,pattern(^[0-9]{11}$)"
                  
                - source-field: "customerId"
                  target-column: "customer_id"
                  type: "STRING"
                  validation: "notNull,pattern(^[0-9]{9}$)"
                  
                - source-field: "activeStatus"
                  target-column: "active_status"
                  type: "BOOLEAN"
                  default: true
                  
                - source-field: "createdAt"
                  target-column: "created_at"
                  type: "TIMESTAMP"
                  default: "CURRENT_TIMESTAMP"
          
          # Step 8: Transaction Data Loading
          - name: "loadTransactionsStep"
            description: "Load transaction data from dailytran.txt with BigDecimal precision"
            next: "loadTransactionCategoryBalancesStep"
            
            tasklet:
              type: "FlatFileToTableTasklet"
              reader-config: "classpath:db/data/transaction-data-reader.yml"
              writer-config: "PostgreSQLBulkInsertWriter"
              
              # Chunk processing configuration
              chunk-size: 1000  # Transaction data chunk size
              commit-interval: 1000
              
              # Error handling configuration
              skip-limit: 50
              skip-policy: "LimitSkipPolicy"
              retry-limit: 3
              
              # File processing configuration
              source-file: "classpath:db/data/ASCII/dailytran.txt"
              target-table: "transactions"
              record-length: 189  # Transaction record length
              
              # Field mapping configuration with BigDecimal precision
              field-mappings:
                - source-field: "transactionId"
                  target-column: "transaction_id"
                  type: "STRING"
                  validation: "notNull,pattern(^[0-9]{10}$)"
                  
                - source-field: "accountId"
                  target-column: "account_id"
                  type: "STRING"
                  validation: "notNull,pattern(^[0-9]{11}$)"
                  
                - source-field: "cardNumber"
                  target-column: "card_number"
                  type: "STRING"
                  validation: "pattern(^[0-9]{16}$)"
                  
                - source-field: "transactionTypeCode"
                  target-column: "transaction_type_code"
                  type: "STRING"
                  validation: "notNull,size(min=2,max=2)"
                  
                - source-field: "categoryCode"
                  target-column: "category_code"
                  type: "STRING"
                  validation: "notNull,size(min=4,max=4)"
                  
                - source-field: "processingCode"
                  target-column: "processing_code"
                  type: "STRING"
                  validation: "notNull,size(min=6,max=6)"
                  
                - source-field: "transactionAmount"
                  target-column: "transaction_amount"
                  type: "BIGDECIMAL"
                  precision: 12
                  scale: 2
                  math-context: "DECIMAL128"
                  rounding-mode: "HALF_EVEN"
                  validation: "decimalMin(-999999999.99),decimalMax(999999999.99)"
                  
                - source-field: "transactionDescription"
                  target-column: "transaction_description"
                  type: "STRING"
                  validation: "notNull,size(min=1,max=100)"
                  
                - source-field: "merchantName"
                  target-column: "merchant_name"
                  type: "STRING"
                  validation: "size(max=50)"
                  
                - source-field: "merchantCity"
                  target-column: "merchant_city"
                  type: "STRING"
                  validation: "size(max=25)"
                  
                - source-field: "merchantZip"
                  target-column: "merchant_zip"
                  type: "STRING"
                  validation: "size(max=10)"
                  
                - source-field: "originalCardNumber"
                  target-column: "original_card_number"
                  type: "STRING"
                  validation: "pattern(^[0-9]{16}$)"
                  
                - source-field: "transactionTimestamp"
                  target-column: "transaction_timestamp"
                  type: "TIMESTAMP"
                  validation: "notNull"
                  format: "yyyy-MM-dd HH:mm:ss.SSSSSS"
                  
                - source-field: "createdAt"
                  target-column: "created_at"
                  type: "TIMESTAMP"
                  default: "CURRENT_TIMESTAMP"
          
          # Step 9: Transaction Category Balances Loading
          - name: "loadTransactionCategoryBalancesStep"
            description: "Load transaction category balances from tcatbal.txt with BigDecimal precision"
            
            tasklet:
              type: "FlatFileToTableTasklet"
              reader-config: "classpath:db/data/reference-data-reader.yml#transaction-category-balances"
              writer-config: "PostgreSQLBulkInsertWriter"
              
              # Chunk processing configuration
              chunk-size: 100  # Transaction category balances chunk size
              commit-interval: 100
              
              # Error handling configuration
              skip-limit: 10
              skip-policy: "LimitSkipPolicy"
              retry-limit: 3
              
              # File processing configuration
              source-file: "classpath:db/data/ASCII/tcatbal.txt"
              target-table: "transaction_category_balances"
              
              # Field mapping configuration with BigDecimal precision
              field-mappings:
                - source-field: "accountId"
                  target-column: "account_id"
                  type: "STRING"
                  validation: "notNull,pattern(^[0-9]{11}$)"
                  
                - source-field: "transactionCategory"
                  target-column: "transaction_category"
                  type: "STRING"
                  validation: "notNull,size(min=4,max=4)"
                  
                - source-field: "categoryBalance"
                  target-column: "category_balance"
                  type: "BIGDECIMAL"
                  precision: 12
                  scale: 2
                  math-context: "DECIMAL128"
                  rounding-mode: "HALF_EVEN"
                  validation: "decimalMin(-999999999.99),decimalMax(999999999.99)"
                  
                - source-field: "lastUpdated"
                  target-column: "last_updated"
                  type: "TIMESTAMP"
                  default: "CURRENT_TIMESTAMP"
                  
                - source-field: "createdAt"
                  target-column: "created_at"
                  type: "TIMESTAMP"
                  default: "CURRENT_TIMESTAMP"
        
        # =====================================================================================
        # JOB-LEVEL CONFIGURATION
        # =====================================================================================
        # Global configuration affecting all steps in the job
        
        # Transaction management configuration
        transaction-manager:
          type: "PlatformTransactionManager"
          isolation-level: "SERIALIZABLE"
          propagation: "REQUIRED"
          timeout: 300  # 5 minutes per transaction
          
        # Job repository configuration
        job-repository:
          type: "PostgreSQLJobRepository"
          table-prefix: "BATCH_"
          isolation-level: "SERIALIZABLE"
          max-varchar-length: 2500
          
        # Error handling configuration
        error-handling:
          global-skip-limit: 500
          global-retry-limit: 3
          exception-mappings:
            - exception: "FlatFileParseException"
              action: "SKIP"
              log-level: "WARN"
            - exception: "ValidationException"
              action: "SKIP"
              log-level: "WARN"
            - exception: "NumberFormatException"
              action: "SKIP"
              log-level: "WARN"
            - exception: "DataIntegrityViolationException"
              action: "RETRY"
              log-level: "ERROR"
            - exception: "DataAccessException"
              action: "RETRY"
              log-level: "ERROR"
            - exception: "DeadlockLoserDataAccessException"
              action: "RETRY"
              log-level: "ERROR"
        
        # Performance monitoring configuration
        monitoring:
          enabled: true
          metrics-enabled: true
          prometheus-enabled: true
          jmx-enabled: true
          progress-reporting:
            enabled: true
            interval: 1000  # Report every 1000 records
            include-memory: true
            include-timing: true
            include-throughput: true
        
        # Security configuration
        security:
          enabled: true
          require-authentication: true
          allowed-roles: "BATCH_USER,ADMIN"
          audit-enabled: true
          
        # Validation configuration
        validation:
          enabled: true
          strict-mode: true
          BigDecimal-precision: 12
          BigDecimal-scale: 2
          date-format: "yyyy-MM-dd"
          timestamp-format: "yyyy-MM-dd HH:mm:ss.SSSSSS"
          
        # COBOL precision configuration
        cobol-precision:
          math-context: "DECIMAL128"
          rounding-mode: "HALF_EVEN"
          strip-trailing-zeros: false
          max-precision: 31
          default-scale: 2
          
        # Bulk loading configuration
        bulk-loading:
          enabled: true
          use-postgresql-copy: true
          batch-size: 1000
          buffer-size: 8192
          concurrent-writers: 3
          
        # Audit configuration
        audit:
          enabled: true
          audit-table: "BATCH_JOB_AUDIT"
          include-job-parameters: true
          include-execution-context: true
          include-performance-metrics: true
          retention-days: 365

# =====================================================================================
# CONFIGURATION PROFILES
# =====================================================================================
# Environment-specific configuration overrides for different deployment environments

---
# Development environment configuration
spring:
  profiles: development
  batch:
    job:
      data-loading-main:
        steps:
          - name: "loadTransactionTypesStep"
            tasklet:
              chunk-size: 5
              commit-interval: 5
          - name: "loadTransactionCategoriesStep"
            tasklet:
              chunk-size: 10
              commit-interval: 10
          - name: "loadDisclosureGroupsStep"
            tasklet:
              chunk-size: 25
              commit-interval: 25
          - name: "loadCustomersStep"
            tasklet:
              chunk-size: 50
              commit-interval: 50
          - name: "loadAccountsStep"
            tasklet:
              chunk-size: 25
              commit-interval: 25
          - name: "loadCardsStep"
            tasklet:
              chunk-size: 50
              commit-interval: 50
          - name: "loadCardXrefStep"
            tasklet:
              chunk-size: 50
              commit-interval: 50
          - name: "loadTransactionsStep"
            tasklet:
              chunk-size: 100
              commit-interval: 100
          - name: "loadTransactionCategoryBalancesStep"
            tasklet:
              chunk-size: 50
              commit-interval: 50
        
        error-handling:
          global-skip-limit: 100
          global-retry-limit: 5
        
        monitoring:
          progress-reporting:
            interval: 100

---
# Staging environment configuration
spring:
  profiles: staging
  batch:
    job:
      data-loading-main:
        steps:
          - name: "loadTransactionTypesStep"
            tasklet:
              chunk-size: 10
              commit-interval: 10
          - name: "loadTransactionCategoriesStep"
            tasklet:
              chunk-size: 20
              commit-interval: 20
          - name: "loadDisclosureGroupsStep"
            tasklet:
              chunk-size: 50
              commit-interval: 50
          - name: "loadCustomersStep"
            tasklet:
              chunk-size: 100
              commit-interval: 100
          - name: "loadAccountsStep"
            tasklet:
              chunk-size: 50
              commit-interval: 50
          - name: "loadCardsStep"
            tasklet:
              chunk-size: 100
              commit-interval: 100
          - name: "loadCardXrefStep"
            tasklet:
              chunk-size: 100
              commit-interval: 100
          - name: "loadTransactionsStep"
            tasklet:
              chunk-size: 500
              commit-interval: 500
          - name: "loadTransactionCategoryBalancesStep"
            tasklet:
              chunk-size: 100
              commit-interval: 100
        
        error-handling:
          global-skip-limit: 250
          global-retry-limit: 3
        
        monitoring:
          progress-reporting:
            interval: 500

---
# Production environment configuration
spring:
  profiles: production
  batch:
    job:
      data-loading-main:
        steps:
          - name: "loadTransactionTypesStep"
            tasklet:
              chunk-size: 10
              commit-interval: 10
          - name: "loadTransactionCategoriesStep"
            tasklet:
              chunk-size: 20
              commit-interval: 20
          - name: "loadDisclosureGroupsStep"
            tasklet:
              chunk-size: 50
              commit-interval: 50
          - name: "loadCustomersStep"
            tasklet:
              chunk-size: 100
              commit-interval: 100
          - name: "loadAccountsStep"
            tasklet:
              chunk-size: 50
              commit-interval: 50
          - name: "loadCardsStep"
            tasklet:
              chunk-size: 100
              commit-interval: 100
          - name: "loadCardXrefStep"
            tasklet:
              chunk-size: 100
              commit-interval: 100
          - name: "loadTransactionsStep"
            tasklet:
              chunk-size: 1000
              commit-interval: 1000
          - name: "loadTransactionCategoryBalancesStep"
            tasklet:
              chunk-size: 100
              commit-interval: 100
        
        error-handling:
          global-skip-limit: 500
          global-retry-limit: 3
        
        monitoring:
          progress-reporting:
            interval: 1000
          
        security:
          audit-enabled: true
          
        bulk-loading:
          enabled: true
          use-postgresql-copy: true
          concurrent-writers: 5

# =====================================================================================
# CONFIGURATION DOCUMENTATION
# =====================================================================================
# 
# This configuration file orchestrates the complete data loading pipeline for the
# CardDemo application, migrating 9 ASCII files from the legacy VSAM system to
# PostgreSQL with exact precision preservation and comprehensive error handling.
# 
# Key Features:
# - Sequential step execution with proper dependency management
# - Exact COBOL COMP-3 decimal precision preservation using BigDecimal
# - Chunk-based processing with configurable chunk sizes
# - Comprehensive error handling with skip and retry policies
# - Performance monitoring and metrics collection
# - Environment-specific configuration profiles
# - Bulk loading optimization using PostgreSQL COPY commands
# - Comprehensive audit logging and compliance tracking
# 
# Performance Characteristics:
# - Processes 9 ASCII files sequentially with proper foreign key dependencies
# - Supports chunk sizes from 50 to 1000 records based on data volume
# - Implements bulk loading for high-volume transaction data
# - Maintains memory usage within 10% increase limit
# - Completes within 4-hour processing window
# 
# Security Features:
# - Role-based access control for job execution
# - Comprehensive audit logging with retention policies
# - PII protection and data masking capabilities
# - Secure credential handling and encryption support
# 
# VSAM Migration Compliance:
# - Preserves exact COBOL numeric precision for financial calculations
# - Maintains referential integrity with foreign key constraints
# - Supports equivalent record locking through SERIALIZABLE isolation
# - Provides restart and recovery capabilities for failed jobs
# =====================================================================================